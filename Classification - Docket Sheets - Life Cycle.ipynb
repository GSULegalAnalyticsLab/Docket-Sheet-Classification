{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT TARGET FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_docketFile():\n",
    "    '''\n",
    "    Input      = None\n",
    "    Operations = Read file, select columns, eliminate time-period = None\n",
    "    Return     = dataframe. \n",
    "    '''\n",
    "    # Read in Excel File\n",
    "    df_docket_sheets = pd.read_excel('2018-02-18 Docket Review.xlsx')\n",
    "    # Select columns\n",
    "    df_docket_sheets_fit = df_docket_sheets.iloc[:,0:-1]\n",
    "    # Select rows != None\n",
    "    TimePeriod_notNone = df_docket_sheets_fit['Time Period'] > 0\n",
    "    # Return dataframe\n",
    "    return df_docket_sheets_fit[TimePeriod_notNone]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONCATENATE TEXT ****** FIX THIS.  YOU DONT NEED TWO FUNCTIONS. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_Concatenated_text_file(Excel_file, New_file_name):     \n",
    "    # Create new write file\n",
    "    New_File = open(str(New_file_name) + '.txt','w')\n",
    "     \n",
    "    # Create Loop Through List of Directories\n",
    "    for row in Excel_file:\n",
    "        \n",
    "        # Write files to new file\n",
    "        New_File.write(str(row))\n",
    "        New_File.write('\\n')\n",
    "    # Close File\n",
    "    New_File.close()\n",
    "\n",
    "def create_concat_docket_text(dataframe):\n",
    "    '''\n",
    "    Input  = dataframe\n",
    "    Output = None, write concatenated text file to cwd\n",
    "    '''\n",
    "    # Select docket text column\n",
    "    df_docket_text = df_docket_sheets_drop_none['docket_text']\n",
    "    # Iterate over rows and concatenate text. Write to file. \n",
    "    create_Concatenated_text_file(df_docket_text, 'Concatenated_Docket_Text')\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT CONCATENATED TEXT FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_concatTxt():\n",
    "    # Import Concat Text File\n",
    "    File = 'Concatenated_Docket_Text.txt'\n",
    "    File_open = open(File) \n",
    "    return File_open.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE SET FROM CONCATENATED TEXT FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_andTokenize_text(Text_file):\n",
    "    '''\n",
    "    Input      = Text File\n",
    "    Operations = Tokenize, lowercase, strip punctuation/stopwords/nonAlpha\n",
    "    Return     = Set of alpha tokens \n",
    "    '''\n",
    "    # Strip Lists\n",
    "    Punct_list = [punct for punct in string.punctuation]\n",
    "    Stopwords = nltk.corpus.stopwords.words('english')\n",
    "    # Tokenize Text\n",
    "    Text_tokenized = nltk.word_tokenize(Text_file)\n",
    "    # Convert tokens to lowercase\n",
    "    Text_lowercase = (token.lower() for token in Text_tokenized)\n",
    "    # Strip Punctuation\n",
    "    Text_tok_stripPunct = filter(lambda x: (x not in Punct_list), Text_lowercase)\n",
    "    # Strip Stopwords\n",
    "    Text_strip_stopWords = filter(lambda x: (x not in Stopwords), Text_tok_stripPunct)\n",
    "    # Strip Non-Alpha\n",
    "    Text_strip_nonAlpha = filter(lambda x: x.isalpha(), Text_strip_stopWords)\n",
    "    # Return set of Docket_text\n",
    "    return Text_strip_nonAlpha\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE DICTIONARY WHOSE KEYS ARE THE FILE CLASSES AND VALUES THE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "You need to create a function that groups the docket sheet dataframe by time period or just sets the dataframe to == the time period in question. \n",
    "\n",
    "Dataframe['State 1']:  Say it has 10 rows.  You'll need to iterate over each row, process the text for that row, create a list to capture the values returned from\n",
    "the if statement = if word in set in clean row text, list.append(1), else list.append(0).  \n",
    "\n",
    "Instead of keeping the lists as lists you should create columns in a dataframe for each.\n",
    "\n",
    "Then you should create a final dataframe to house a column that is the average for each row. Each stage should have its own column.\n",
    "\n",
    "Return that final dataframe. \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE WORD FREQUENCY DISTRIBUTION FOR EACH DOCKET SHEET LIFE CYCLE STAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Master Docket Sheet File\n",
    "df_Master_DocketSheet_File = import_docketFile()\n",
    "\n",
    "# Create Concat Text File\n",
    "#create_concat_docket_text(df_docket_sheets)\n",
    "\n",
    "# Import Back Into Memory Concatenated Text\n",
    "docket_sheet_concatenatedText_File = import_concatTxt()\n",
    "\n",
    "# Create Set of Words From Concatenated File\n",
    "Clean_tokenized_concatText = clean_andTokenize_text(docketSheet_concatenatedText_File)\n",
    "Set_tokenized_concatText = list(set(Clean_tokenized_concatText))\n",
    "\n",
    "# Create Dataframe with Index as set of word\n",
    "df = pd.DataFrame(Set_tokenized_concatText)\n",
    "df_docketsheet_wordSet = df.set_index(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FREQUENCY DISTRIBUTION FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_freq_Dist_setWord_Single_timePeriod(df_docketSheet_single_TimePeriod, df_docketsheet_wordSet):\n",
    "    '''The purpose of this function is to calculate the frequency with which each setword appears in a single lifecycle time period. \n",
    "    Input  = df_docketsheet_wordSet, docket_sheet_dataframe\n",
    "    Output = List with each value representing the avg that each setWord appears in the text. \n",
    "    '''\n",
    "    \n",
    "    # List to capture the average count each set_word appears in the docket sheet.  \n",
    "    List_avg_appearance_set_word = []\n",
    "    \n",
    "    # For each word_set in the word_set index:\n",
    "    for word_set in df_docketsheet_wordSet.index:\n",
    "        \n",
    "        # Define your counts to capture. \n",
    "        Count_num_rows = 0\n",
    "        Count_matches = 0\n",
    "        \n",
    "        # Iterate over each row in the docket sheet, all of which are part of the same life cycle period.  \n",
    "        for row in df_docketSheet_single_TimePeriod.itertuples():\n",
    "            \n",
    "            # Keep count of each token list we iterate over.  This will be the denominator for calculating our average. \n",
    "            Count_num_rows += 1\n",
    "\n",
    "            # Run clearning and tokenizing function over each row.  Return list of clean tokens. \n",
    "            clean_tokenized_text = list(clean_andTokenize_text(row[4]))\n",
    "            \n",
    "            # Check to see if in the given row there is a match. \n",
    "            if word_set in clean_tokenized_text:\n",
    "                # Count the match. \n",
    "                Count_matches +=1\n",
    "        \n",
    "        # Calculate the avg times the set word appears in the text.  \n",
    "        '''Note that we are now outside of the loop to iterate over the rows, bust still within the word_set loop. \n",
    "           Therefore, this will calculate the avg for each setWord'''\n",
    "        \n",
    "        Avg = round((Count_matches / Count_num_rows),2)\n",
    "        \n",
    "        # Append the average value to the list.  Each value will coinside with a setWord.  \n",
    "        #Since we are within the loop the order between setWord and value will be kept. \n",
    "        List_avg_appearance_set_word.append(Avg)\n",
    "    \n",
    "    # Return to the user the List containing the average appearance of each setWord. \n",
    "    return List_avg_appearance_set_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Life Cycle Stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "List_LC_stages = set(df_Master_DocketSheet_File['Time Period'])\n",
    "\n",
    "# Iterate over Life Cycle Stages\n",
    "for stage in List_LC_stages:\n",
    "    # Limit the Master DocketSheet File to the current stage\n",
    "    Match_period = df_Master_DocketSheet_File['Time Period'] == stage\n",
    "    df_docketSheet_stage_N = df_Master_DocketSheet_File[Match_period]\n",
    "    \n",
    "    # Get the Frequency Distribution of setWords for the current Stage\n",
    "    List_setWord_freqDist = get_freq_Dist_setWord_Single_timePeriod(df_docketSheet_stage_N, df_docketsheet_wordSet)\n",
    "    \n",
    "    \n",
    "    # Create a Column in the Master Dataframe representing the frequency Distribution for each LC Stage\n",
    "    \n",
    "    df_docketsheet_wordSet['Life Cycle Stage: '+str(stage)] = List_setWord_freqDist \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_excel(dataframe, filename):\n",
    "    import pandas as pd\n",
    "    writer = pd.ExcelWriter(filename+'.xlsx')\n",
    "    dataframe.to_excel(writer, sheet_name = 'Data')\n",
    "    writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_to_excel(df_docketsheet_wordSet, 'DocketSheet_wordSet_Freq_Dist')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Create a dictionary whose keys are the file classes and values the files for each group. \n",
    "    Dict_files_by_class = create_file_list_by_group(Excel_file)\n",
    "    \n",
    "    # Create a Master Dataframe to house final values\n",
    "    Master_dataframe = pd.DataFrame({}, index = list(dataframe_unique_tokens.index))\n",
    "    \n",
    "    # Loop over the dictionary keys\n",
    "    for Class in Dict_files_by_class.keys():\n",
    "        \n",
    "        # Create a list of the files\n",
    "        File_list = list(Dict_files_by_class[Class])\n",
    "        \n",
    "        # Feed each class list trough the word counter\n",
    "        df_single_class_word_count = get_frequencyDist_legal_text(File_list, dataframe_unique_tokens)\n",
    "        \n",
    "        # Merge individual text file columns into one percentage for the given class\n",
    "        df_merged_columns = Merge_dataframe_columns_calc_percentage(Class, df_single_class_word_count)\n",
    "        \n",
    "        # Merge individual dataframes into master dataframe\n",
    "        Master_dataframe[Class] = df_merged_columns[Class]\n",
    "        \n",
    "    # Return the Master Dataframe with the word % for all classes. \n",
    "    return Master_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
